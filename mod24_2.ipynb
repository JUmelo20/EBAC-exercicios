{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25944ec5-58a4-4698-9ac8-141bde8f5a56",
   "metadata": {},
   "source": [
    "## 1. Cite 5 diferenças entre o AdaBoost e o GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9830ba-b984-405a-b686-a0489e7d17fc",
   "metadata": {},
   "source": [
    "**Algoritmos base:** AdaBoost utiliza classificadores fracos, como árvores de decisão, para construir o conjunto final, enquanto GBM utiliza árvores de decisão como base para construir o conjunto iterativamente.\n",
    "\n",
    "**Peso das amostras:** AdaBoost ajusta os pesos das amostras a cada iteração, dando maior importância às amostras classificadas incorretamente anteriormente. GBM ajusta os pesos das amostras a cada iteração, mas com base nos resíduos dos modelos anteriores.\n",
    "\n",
    "**Processo de treinamento:** AdaBoost treina os classificadores fracos sequencialmente, ajustando os pesos das amostras a cada iteração. GBM treina os modelos iterativamente, onde cada modelo tenta corrigir os erros dos modelos anteriores.\n",
    "\n",
    "**Sensibilidade ao ruído:** AdaBoost é mais sensível ao ruído nos dados de treinamento, pois ajusta os pesos das amostras a cada iteração. GBM é menos sensível ao ruído, pois os modelos subsequentes tentam corrigir os erros dos modelos anteriores.\n",
    "\n",
    "**Taxa de aprendizado:** AdaBoost utiliza uma taxa de aprendizado fixa para controlar a contribuição de cada classificador fraco. GBM utiliza uma taxa de aprendizado para controlar a contribuição de cada modelo em cada iteração."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f421bc20-51bf-4bf0-8d80-e97ee9ab96f8",
   "metadata": {},
   "source": [
    "## 2. Exemplo de classificação e de regressão do GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd2ef6b6-3327-42f7-a5ae-a9b5c9dc5592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 100.00%\n",
      "Erro quadrático médio: 6.21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris, load_boston\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Exemplo de classificação com Gradient Boosting\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data\n",
    "y = iris_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Acurácia: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Exemplo de regressão com Gradient Boosting\n",
    "boston_data = load_boston()\n",
    "X = boston_data.data\n",
    "y = boston_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_reg.fit(X_train, y_train)\n",
    "y_pred = gb_reg.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Erro quadrático médio: {:.2f}\".format(mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06654d3a-be26-4a13-abb8-b8203890346c",
   "metadata": {},
   "source": [
    "## 3. Cite 5 Hyperparametros importantes no GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a121436e-8138-4309-96a2-3459ef0715a6",
   "metadata": {},
   "source": [
    "**n_estimators:** O número de modelos de boosting a serem criados.\n",
    "\n",
    "**learning_rate:** A taxa de aprendizado, controla a contribuição de cada modelo ao conjunto.\n",
    "\n",
    "**max_depth:** A profundidade máxima das árvores de decisão.\n",
    "\n",
    "**subsample:** A fração de amostras a serem usadas para o treinamento de cada árvore. Um valor menor reduz a variância, mas também pode levar a um viés maior.\n",
    "\n",
    "**loss:** A função de perda a ser otimizada durante o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f13428-8cd8-49f4-9954-296f0c9f784f",
   "metadata": {},
   "source": [
    "## 4.  Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7045f134-28ae-4372-9b11-8c759b0d9d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros:\n",
      "{'learning_rate': 0.1, 'max_depth': None, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Melhor pontuação:\n",
      "0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Carregar o conjunto de dados iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Criar o classificador Gradient Boosting\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "# Definir os hiperparâmetros para ajustar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.5, 1.0],\n",
    "    'max_depth': [3, 5, None],\n",
    "    'subsample': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Realizar a busca de hiperparâmetros utilizando validação cruzada\n",
    "grid_search = GridSearchCV(gb_clf, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Exibir os melhores hiperparâmetros encontrados\n",
    "print(\"Melhores hiperparâmetros:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Exibir a melhor pontuação alcançada durante a busca de hiperparâmetros\n",
    "print(\"Melhor pontuação:\")\n",
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183087aa-636c-403a-93f3-f72c0f582fc7",
   "metadata": {},
   "source": [
    "## 5. Qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92afc8f-7d33-4ab3-a8a5-b52e8724a932",
   "metadata": {},
   "source": [
    "A maior diferença entre o Gradient Boosting e o Gradient Boosting Estocástico (Stochastic Gradient Boosting) é que o Stochastic Gradient Boosting introduz uma aleatoriedade adicional ao amostrar as instâncias de treinamento para cada modelo. Isso significa que cada modelo é ajustado em uma amostra aleatória diferente das instâncias de treinamento, o que adiciona uma abordagem estocástica ao processo de treinamento. Isso pode ajudar a reduzir o sobreajuste e melhorar a generalização do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f67726-4f1c-4eea-88d7-726c8637e0af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
